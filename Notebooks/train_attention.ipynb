{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using Kaggle to download the dataset"
   ],
   "metadata": {
    "id": "Q4hwitVbmWVy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import kagglehub\n",
    "\n",
    "files.upload()"
   ],
   "metadata": {
    "id": "GkZuyFyCkk9Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "path = kagglehub.dataset_download(\"eeshawn/flickr30k\")\n",
    "print(path)"
   ],
   "metadata": {
    "id": "4iQoC6fOMMxP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5a7a8d41-5870-4156-d392-6e7a93bd9482"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Declaring Addresses\n"
   ],
   "metadata": {
    "id": "DMbiUCFgRteG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delaring Addresses"
   ],
   "metadata": {
    "id": "UCzUW6RSTCjN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os, shutil, glob\n",
    "\n",
    "drive_base = \"/content/drive/MyDrive/captioneer\"\n",
    "drive_scripts = os.path.join(drive_base, \"scripts\")\n",
    "drive_captions = os.path.join(drive_base, \"processed_captions\")\n",
    "\n",
    "colab_scripts = \"/content/scripts\"\n",
    "colab_captions = \"/content/processed_captions\"\n",
    "colab_images = os.path.join(path, \"flickr30k_images\")\n",
    "\n",
    "os.makedirs(colab_scripts, exist_ok=True)\n",
    "os.makedirs(\"/content/models\", exist_ok=True)"
   ],
   "metadata": {
    "id": "8uNRy2lFRsNo"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Copying Scripts"
   ],
   "metadata": {
    "id": "W12NiiVkS_dt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for fname in [\"dataset_attention.py\", \"model_attention.py\", 'train_attention.py']:\n",
    "    src = os.path.join(drive_scripts, fname)\n",
    "    dst = os.path.join(colab_scripts, fname)\n",
    "    if os.path.exists(dst):\n",
    "        os.remove(dst)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "        print(\"Copied\", fname, \"to\", dst)\n",
    "    else:\n",
    "        print(\"not found in drive:\", src)\n",
    "print(\"Scripts placed in\", colab_scripts)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pk0inm2LTU13",
    "outputId": "4e846269-b7eb-4eb1-be31-2368145f46ff"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Copying `processed_captions`"
   ],
   "metadata": {
    "id": "rcZ9-tkSRSmW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "# copy captions folder\n",
    "if os.path.exists(colab_captions):\n",
    "    print(\"Removing existing\", colab_captions)\n",
    "    shutil.rmtree(colab_captions)\n",
    "shutil.copytree(drive_captions, colab_captions)\n",
    "print(\"Copied processed captions to\", colab_captions)\n",
    "\n",
    "# quick checks\n",
    "images = glob.glob(os.path.join(colab_images, \"*.jpg\"))\n",
    "print(\"Number of images:\", len(images))\n",
    "json_files = glob.glob(os.path.join(colab_captions, \"*.json\"))\n",
    "print(\"Processed captions files:\", json_files)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNPCxkyCQzrL",
    "outputId": "89cfae03-0a7f-48a3-d3af-82ce54502249"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "id": "azp9tFQKT1Pj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader Creation"
   ],
   "metadata": {
    "id": "2ihzJerkT2fI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataloader creation (uses dataset_attention.py)\n",
    "import sys\n",
    "sys.path.append('/content/scripts')\n",
    "from importlib import reload\n",
    "import dataset_attention\n",
    "reload(dataset_attention)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_seq = os.path.join(colab_captions, \"sequences_train.json\")\n",
    "val_seq = os.path.join(colab_captions, \"sequences_val.json\")\n",
    "vocab_path = os.path.join(colab_captions, \"vocab_word2idx.json\")\n",
    "\n",
    "# dataset + loader (small batch for checking)\n",
    "batch_size = 8\n",
    "ds = dataset_attention.CaptionImageDataset(train_seq, colab_images, vocab_json=vocab_path, image_size=224)\n",
    "loader = DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=dataset_attention.collate_fn, num_workers=2)\n",
    "\n",
    "# inspect one batch\n",
    "images, seqs, names = next(iter(loader))\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Seqs shape:\", seqs.shape)\n",
    "print(\"Sample image names:\", names[:4])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUpQBW2jT4S5",
    "outputId": "8512f749-2d45-4cc0-ca27-bfe450984be4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing and Preparation\n",
    "double checking libraries and paths and scripts"
   ],
   "metadata": {
    "id": "SRdhVXHWUaVd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Importing and Preparation\n",
    "import os, sys, json, torch\n",
    "from importlib import reload\n",
    "import model_attention, train_attention\n",
    "reload(model_attention)\n",
    "reload(train_attention)\n",
    "\n",
    "# paths\n",
    "colab_captions = \"/content/processed_captions\"\n",
    "colab_images = os.path.join(path, \"flickr30k_images\")\n",
    "vocab_path = os.path.join(colab_captions, \"vocab_word2idx.json\")\n",
    "idx2word_path = os.path.join(colab_captions, \"vocab_idx2word.json\")\n",
    "train_seq = os.path.join(colab_captions, \"sequences_train.json\")\n",
    "val_seq = os.path.join(colab_captions, \"sequences_val.json\")\n",
    "test_seq = os.path.join(colab_captions, \"sequences_test.json\")\n",
    "models_dir = \"/content/models\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# load vocab jsons\n",
    "with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)\n",
    "with open(idx2word_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_idx2 = json.load(f)\n",
    "idx2word = {int(k): v for k, v in raw_idx2.items()}\n",
    "\n",
    "start_token = word2idx.get(\"<START>\", 2)\n",
    "end_token = word2idx.get(\"<END>\", 3)\n",
    "pad_token = word2idx.get(\"<PAD>\", 0)\n",
    "print(\"Vocab size:\", len(word2idx))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-i9hqfNUVMf",
    "outputId": "2ee29e59-d2ee-465f-b4c8-576844f21b41"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sanity Check"
   ],
   "metadata": {
    "id": "_ZLSv_bkUrY_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "feat_dim = 2048\n",
    "feat_embed_dim = 512\n",
    "embed_dim = 512\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.5\n",
    "\n",
    "enc = model_attention.EncoderCNN(feat_dim=feat_dim, feat_embed_dim=feat_embed_dim, train_backbone=False).to(device)\n",
    "dec = model_attention.DecoderWithAttention(embed_dim=embed_dim, hidden_dim=hidden_dim,\n",
    "                                 vocab_size=len(word2idx), feat_dim=feat_dim,\n",
    "                                 feat_embed_dim=feat_embed_dim, attn_dim=512,\n",
    "                                 num_layers=num_layers, dropout=dropout).to(device)\n",
    "\n",
    "# single-batch forward pass (sanity)\n",
    "images, seqs, names = next(iter(loader))\n",
    "images = images.to(device)\n",
    "seqs = seqs.to(device)\n",
    "\n",
    "enc.train(); dec.train()\n",
    "feats, global_feat = enc(images)\n",
    "print(\"Encoder feats shape:\", feats.shape, \"global shape:\", global_feat.shape)\n",
    "logits = dec(feats, seqs)\n",
    "print(\"Decoder logits shape:\", logits.shape)\n",
    "\n",
    "# simple loss step\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "B, Lm1, V = logits.shape\n",
    "loss = criterion(logits.view(B*Lm1, V), seqs[:,1:].reshape(-1))\n",
    "print(\"One-step train loss:\", loss.item())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejgIRjHtUsjO",
    "outputId": "e96bbffb-63b8-46c3-8ee5-af9df2b4cd49"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "reload(train_attention)\n",
    "\n",
    "config = {\n",
    "    'train_sequences': train_seq,\n",
    "    'val_sequences': val_seq,\n",
    "    'images_dir': colab_images,\n",
    "    'vocab_path': vocab_path,\n",
    "    'save_dir': models_dir,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'batch_size': 256,\n",
    "    'epochs': 50,\n",
    "    'lr': 1e-3,\n",
    "    'patience': 7,\n",
    "    'embed_dim': embed_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout,\n",
    "    'feat_dim': feat_dim,\n",
    "    'feat_embed_dim': feat_embed_dim,\n",
    "    'max_len': 30,\n",
    "    'resume_path': \"/content/drive/MyDrive/captioneer/models_attention/best_model_epoch13_bleu0.2031.pt\"\n",
    "}\n",
    "\n",
    "history = train_attention.train_loop(config)"
   ],
   "metadata": {
    "id": "XkCUYzVuXTuW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing Losses & Scores"
   ],
   "metadata": {
    "id": "DBgrsU3McOG4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history)\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot BLEU scores\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history[\"val_bleu\"], marker=\"o\", label=\"Val BLEU\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"BLEU Score\")\n",
    "plt.title(\"Validation BLEU Evolution\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "sypn1PJAfFdd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Best Model And Check Generated Captions VS Ground Truth"
   ],
   "metadata": {
    "id": "D1naUoiaY5Rb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "\n",
    "reload(model_attention)\n",
    "reload(train_attention)\n",
    "\n",
    "# Find the best model checkpoint\n",
    "list_of_files = glob.glob(os.path.join(models_dir, '*.pt'))\n",
    "if not list_of_files:\n",
    "    print(\"No trained models found in the directory.\")\n",
    "else:\n",
    "    best_model_path = max(list_of_files, key=os.path.getctime)\n",
    "    print(f\"Loading best model: {best_model_path}\")\n",
    "\n",
    "    # Initialize models\n",
    "    encoder = model_attention.EncoderCNN(\n",
    "        feat_dim=config['feat_dim'],\n",
    "        feat_embed_dim=config['feat_embed_dim']\n",
    "    ).to(device)\n",
    "    decoder = model_attention.DecoderWithAttention(\n",
    "        embed_dim=config['embed_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        vocab_size=len(word2idx),\n",
    "        feat_embed_dim=config['feat_embed_dim']\n",
    "    ).to(device)\n",
    "\n",
    "    # Load state dictionaries\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state'])\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    test_ds = dataset_attention.CaptionImageDataset(\n",
    "        test_seq, colab_images, vocab_path, is_val=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=1, shuffle=True, collate_fn=dataset_attention.collate_fn\n",
    "    )\n",
    "\n",
    "    def sequence_to_text(seq, idx2word_map):\n",
    "        return ' '.join([idx2word_map[idx] for idx in seq if idx not in {start_token, end_token, pad_token}])\n",
    "\n",
    "    # Generate and display captions\n",
    "    num_samples_to_show = 5\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (image_t, caps_t_list, img_name) in enumerate(test_loader):\n",
    "            if i >= num_samples_to_show:\n",
    "                break\n",
    "\n",
    "            image_t = image_t.to(device)\n",
    "\n",
    "            # Generate caption using beam search\n",
    "            feats, _ = encoder(image_t)\n",
    "            prediction_idx = decoder.beam_search_decode(feats, start_token, end_token)[0]\n",
    "            predicted_caption = sequence_to_text(prediction_idx, idx2word)\n",
    "\n",
    "            # Prepare ground truth captions\n",
    "            gt_captions = [sequence_to_text(cap.squeeze().tolist(), idx2word) for cap in caps_t_list[0]]\n",
    "\n",
    "            # Display the image and captions\n",
    "            img_display = image_t.squeeze(0).cpu().numpy().transpose((1, 2, 0))\n",
    "            img_display = std * img_display + mean\n",
    "            img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(img_display)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Generated Caption:\\n> {predicted_caption}\\n\", loc='left')\n",
    "\n",
    "            gt_text = \"Ground Truth Captions:\\n\" + \"\\n\".join([f\"- {cap}\" for cap in gt_captions])\n",
    "            plt.figtext(0.5, -0.1, gt_text, ha=\"center\", fontsize=10, wrap=True)\n",
    "\n",
    "            plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2fyee6FegoFn",
    "outputId": "c53aa177-7bd7-4060-88f4-4c6bedc18d18"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
